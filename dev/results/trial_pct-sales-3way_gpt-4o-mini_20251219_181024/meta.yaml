name: pct-sales-3way
model: gpt-4o-mini
timestamp: 2025-12-19 18:10:24
n_runs: 3

# Variant A: no preview (baseline)
# Variant B: with preview (LLM sees result)
# Variant C: with validation (retry if no valid result)

durations_a: [
  15.8, 11.9, 11.4
]
durations_b: [
  31.8, 16, 18.8
]
durations_c: [
  17.1, 13.8, 9.5
]

tool_calls_a: [
  1, 1, 1
]
tool_calls_b: [
  3, 1, 3
]
tool_calls_c: [
  1, 2, 1
]

validation_retries_c: [
  0, 0, 0
]

# Quick summary: how many produced a valid data.frame?
has_result_a: 2/3
has_result_b: 2/3
has_result_c: 3/3

# ============================================================
# EVALUATION BY CLAUDE
# ============================================================

evaluation:
  run_01_a:
    correct: false
    reason: "No eval_tool called - only data_tool. No validated result."

  run_01_b:
    correct: true
    pct_sum: 0.99
    reason: "All requirements met. pct sums to ~1.0 (rounding)."

  run_01_c:
    correct: true
    pct_sum: 0.99
    reason: "All requirements met. Single eval_tool call, efficient."

  run_02_a:
    correct: false
    pct_sum: 5.0
    reason: "BUG: pct_of_total all show 1.0. Computed inside summarize() while grouped."

  run_02_b:
    correct: false
    reason: "No eval_tool called - only data_tool. No validated result."

  run_02_c:
    correct: true
    pct_sum: 1.0
    reason: "All requirements met. Correct approach: compute pct after ungroup."

  run_03_a:
    correct: false
    pct_sum: 5.0
    reason: "BUG: pct_of_total all show 1.0. Same grouped summarize bug."

  run_03_b:
    correct: true
    pct_sum: 0.99
    reason: "Fixed across() error on retry. Final result correct."

  run_03_c:
    correct: false
    pct_sum: 5.0
    reason: "BUG: pct_of_total all show 1.0. Computed inside summarize() while grouped."

summary:
  variant_a: "0/3 correct (1 no result, 2 wrong pct)"
  variant_b: "2/3 correct (1 no result)"
  variant_c: "2/3 correct (1 wrong pct)"

  observations: |
    - Validation loop (C) ensures we always get a result (3/3 vs 2/3)
    - But validation only checks for data.frame existence, not correctness
    - Common bug: computing pct_of_total inside summarize() while grouped
      gives 1.0 per row (sum is per-group, not global)
    - Variant B (preview) helps LLM catch issues but still misses validation sometimes
    - Variant C is fastest when it works (avg 13.5s vs 22.2s for B)
    - Key insight: need semantic validation, not just "is it a data.frame?"

  recommendations: |
    - Add semantic validation: check if pct_of_total sums to ~1.0
    - Or use preview (B) + validation (C) together
    - Current validation only guarantees a result, not correctness
